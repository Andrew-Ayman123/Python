{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze A/B Test Results \n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "- [Conclusion](#Conclusion)\n",
    "\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists. For this project, We will be working to understand the results of an A/B test run by an e-commerce website. Our goal is to work through this notebook to help the company understand if they should:\n",
    "- Implement the new webpage, \n",
    "- Keep the old webpage, or \n",
    "- Perhaps run the experiment longer to make their decision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='probability'></a>\n",
    "## Part I - Probability\n",
    "\n",
    "To get started, let's import our libraries.\n",
    "\n",
    "`pandas` package offers Data Manipulation and analysis using Dataframes and Series.</br>\n",
    "`numpy`  package offers Data Manipulation, speed reliable mathematical functions.</br>\n",
    "`random` package offers useful randomness functions including configuring the seed.</br>\n",
    "`matplotlib.pyplot` module offers Data Visualization to draw useful and insightful charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#We are setting the seed to assure you get the same answers \n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We read in the `ab_data.csv` data. Store it in `df`. Below is the description of the data, there are a total of 5 columns:\n",
    "\n",
    "<center>\n",
    "\n",
    "|Data columns|Purpose|Valid values|\n",
    "| ------------- |:-------------| -----:|\n",
    "|user_id|Unique ID|Int64 values|\n",
    "|timestamp|Time stamp when the user visited the webpage|-|\n",
    "|group|In the current A/B experiment, the users are categorized into two broad groups. <br>The `control` group users are expected to be served with `old_page`; and `treatment` group users are matched with the `new_page`. <br>However, **some inaccurate rows** are present in the initial data, such as a `control` group user is matched with a `new_page`. |`['control', 'treatment']`|\n",
    "|landing_page|It denotes whether the user visited the old or new webpage.|`['old_page', 'new_page']`|\n",
    "|converted|It denotes whether the user decided to pay for the company's product. Here, `1` means yes, the user bought the product.|`[0, 1]`|\n",
    "</center>\n",
    "\n",
    "**a.** Read in the dataset from the `ab_data.csv` file and take a look at the top few rows here:\n",
    "\n",
    "`read_csv()` to read a csv file and transform into a Dataframe.</br>\n",
    "`.head()` method to display the top 5 rows of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Use the cell below to find the number of rows in the dataset.\n",
    "\n",
    "`shape` attribute produces the tuple`(nrows,ncolumns)` then we specify the rows by index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** The number of unique users in the dataset.\n",
    "\n",
    "We specify the `['user_id']` column first then call `.nunique()` method to know how many unique users we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**d.** The proportion of users converted.\n",
    "\n",
    "We could take the `mean` of the `converted` column because it's only one when it's converted.</br>\n",
    "instead of getting the count of converted and dividing by the length of all of them</br>\n",
    "We multiplied by 100 to get the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.96591935560551"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df['converted']==1].shape[0]/df.shape[0]*100\n",
    "df['converted'].mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**e.** The number of times when the \"group\" is `treatment` but \"landing_page\" is not a `new_page`.\n",
    "\n",
    ">**The number of times the 'new_page' and 'treatment' don't match means the number of times new_page and treatment don't match plus the number old_page and control don't match.**\n",
    "\n",
    "`.query()` method to select the rows where `group = \"treatment\" and landing_page = \"old_page\" ` and adds to the `landing_page==\"new_page\" and group==\"control\"`.<br>\n",
    "`shape` attribute produces the tuple`(nrows,ncolumns)` then we specify the rows by index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(landing_page==\"old_page\" and group==\"treatment\") or (landing_page==\"new_page\" and group==\"control\")').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f.** Do any of the rows have missing values?\n",
    "\n",
    "`.info()` method to find out how many non null values in each column.<br>\n",
    "As we can see they are all the same so no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294478 non-null  int64 \n",
      " 1   timestamp     294478 non-null  object\n",
      " 2   group         294478 non-null  object\n",
      " 3   landing_page  294478 non-null  object\n",
      " 4   converted     294478 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a particular row, the **group** and **landing_page** columns should have either of the following acceptable values:\n",
    "\n",
    "|user_id| timestamp|group|landing_page|converted|\n",
    "|---|---|---|---|---|\n",
    "|XXXX|XXXX|`control`| `old_page`|X |\n",
    "|XXXX|XXXX|`treatment`|`new_page`|X |\n",
    "\n",
    "\n",
    "It means, the `control` group users should match with `old_page`; and `treatment` group users should matched with the `new_page`. \n",
    "\n",
    "However, for the rows where `treatment` does not match with `new_page` or `control` does not match with `old_page`, we cannot be sure if such rows truly received the new or old wepage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Now use the answer to the quiz to create a new dataset that meets the specifications from the quiz.  Store your new dataframe in **df2**.\n",
    "\n",
    "`.query()` method to select the rows where `group == \"treatment\" and landing_page == \"new_page\" `**or** `group == \"treatment\" and landing_page == \"new_page\" `<br>\n",
    "`.head()` method to display the top 5 rows of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the inaccurate rows, and store the result in a new dataframe df2\n",
    "df2=df.query('(group == \"control\" and landing_page == \"old_page\") or (group == \"treatment\" and landing_page == \"new_page\")')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we cleared every false valued row using another method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the incorrect rows were removed from df2 - \n",
    "# Output of the statement below should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** How many unique **user_id**s are in **df2**?\n",
    "\n",
    "We specify the `['user_id']` column first then call `.nunique()` method to know how many unique users we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**b.** There is one **user_id** repeated in **df2**.  What is it?\n",
    "\n",
    "To find the duplicated id we can call `.mode()` method to find what occurred more than once in the `user_id` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    773192\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_id = df2['user_id'].mode()\n",
    "duplicated_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Display the rows for the duplicate **user_id**? \n",
    "\n",
    "`.query()` method to extract the rows with duplicated id.<br>\n",
    "Using a fstring to use the `duplicated_id` directly, using the attribute `values` to access the value within the Series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.query(f'user_id == {duplicated_id.values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Remove **one** of the rows with a duplicate **user_id**, from the **df2** dataframe.\n",
    "\n",
    "`.drop_duplicated()` method to drop the duplicated values within the column specified `'user_id'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove one of the rows with a duplicate user_id..\n",
    "df2=df2.drop_duplicates('user_id')\n",
    "\n",
    "# Check again if the row with a duplicate user_id is deleted or not\n",
    "df2.query(f'user_id == {duplicated_id.values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** What is the probability of an individual converting regardless of the page they receive?<br><br>\n",
    "\n",
    "calling `.mean()` method on the `converted` column to find out the probability required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Given that an individual was in the `control` group, what is the probability they converted?\n",
    "\n",
    "First we specify the `control` group then call `.mean()` on it's `converted` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_mean=df2[df2['group']=='control']['converted'].mean()\n",
    "control_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Given that an individual was in the `treatment` group, what is the probability they converted?\n",
    "\n",
    "First we specify the `treatment` group then call `.mean()` on it's `converted` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_mean=df2[df2['group']=='treatment']['converted'].mean()\n",
    "treatment_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the actual difference (obs_diff) between the conversion rates for the two groups.\n",
    "obs_diff=treatment_mean-control_mean\n",
    "obs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** What is the probability that an individual received the new page?<br>\n",
    "We divide the nRows of the `new_page` over the whole rows of `df2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2['landing_page']=='new_page'].shape[0]/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** Consider your results from parts (a) through (d) above, and explain below whether the new `treatment` group users lead to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the surface it feels like it doesn't lead to new conversions but we have to find out p value first to tell if it's significant or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "## Part II - A/B Test\n",
    "\n",
    "Since a timestamp is associated with each event, you could run a hypothesis test continuously as long as you observe the events. \n",
    "\n",
    "However, then the hard questions would be: \n",
    "- Do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  \n",
    "- How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For now, consider you need to make the decision just based on all the data provided.  \n",
    "\n",
    "If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should be your null and alternative hypotheses (**$H_0$** and **$H_1$**)?  \n",
    "\n",
    "You can state your hypothesis in terms of words or in terms of **$p_{old}$** and **$p_{new}$**, which are the \"converted\" probability (or rate) for the old and new pages respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Based on all the data and `obs_diff` < 0 that the old page is better but we need more investigation to make sure.<br>\n",
    ">**$H_0$** is that the old page is better. (**$p_{new}$** - **$p_{old}$** <= 0) <br>\n",
    ">**$H_1$** is that new page proves to be definitely better. (**$p_{new}$** - **$p_{old}$** > 0)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Hypothesis $H_0$ Testing\n",
    "Under the null hypothesis $H_0$, assume that $p_{new}$ and $p_{old}$ are equal. Furthermore, assume that $p_{new}$ and $p_{old}$ both are equal to the **converted** success rate in the `df2` data regardless of the page. So, our assumption is: <br><br>\n",
    "$p_{new}$ = $p_{old}$ = $p_{population}$\n",
    "\n",
    "\n",
    "In this section, We will: \n",
    "\n",
    "- Simulate (bootstrap) sample data set for both groups, and compute the  \"converted\" probability $p$ for those samples. \n",
    "\n",
    "\n",
    "- Use a sample size for each group equal to the ones in the `df2` data.\n",
    "\n",
    "\n",
    "- Compute the difference in the \"converted\" probability for the two samples above. \n",
    "\n",
    "\n",
    "- Perform the sampling distribution for the \"difference in the converted probability\" between the two simulated-samples over 10,000 iterations; and calculate an estimate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** What is the **conversion rate** for $p_{new}$ under the null hypothesis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $p_{new}$ = $p_{old}$ = $p_{population}$ so `p_new` and `p_old` are the same as the mean of the `converted` column of `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_new=df2['converted'].mean()\n",
    "p_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** What is the **conversion rate** for $p_{old}$ under the null hypothesis? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_old=df2['converted'].mean()\n",
    "p_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** What is $n_{new}$, the number of individuals in the treatment group? <br><br>\n",
    "We find out the number of them by querying the `new_page` and getting the nRows from the `shape` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_new=df2.query('landing_page == \"new_page\"').shape[0]\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** What is $n_{old}$, the number of individuals in the control group?\n",
    "\n",
    "We find out the number of them by querying the `old_page` and getting the nRows from the `shape` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_old=df2.query('landing_page == \"old_page\"').shape[0]\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. Simulate Sample for the `treatment` Group**<br> \n",
    "Simulate $n_{new}$ transactions with a conversion rate of $p_{new}$ under the null hypothesis.  \n",
    "\n",
    "We generate a random sample from our treatment group with replaced data.<br>\n",
    "`choice` function to generate an array with the size of `n_new` and the probability of 1 is as the original group `p_new`<br>\n",
    "We could also use `df2.sample(n_new,replace=True)['converted']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12063863464317666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page_converted=np.random.choice([1,0],size=n_new,p=[p_new,1-p_new])\n",
    "new_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. Simulate Sample for the `control` Group** <br>\n",
    "Simulate $n_{old}$ transactions with a conversion rate of $p_{old}$ under the null hypothesis. <br> \n",
    "\n",
    "We generate a random sample from our control group with replaced data.<br>\n",
    "`choice` function to generate an array with the size of `n_old` and the probability of 1 is as the original group `p_old`<br>\n",
    "We could also use `df2.sample(n_old,replace=True)['converted']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1192987045169817"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_page_converted=np.random.choice([1,0],size=n_old,p=[p_old,1-p_old])\n",
    "old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g.** Find the difference in the \"converted\" probability $(p{'}_{new}$ - $p{'}_{old})$ for your simulated samples.<br>\n",
    "\n",
    "We subtract the two means of our two samples and it's as we hypothesized that the difference equals zero<br>\n",
    ">It will never equals a perfect zero but it tends to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013399301261949603"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif=new_page_converted.mean()-old_page_converted.mean()\n",
    "dif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**h. Sampling distribution** <br>\n",
    "Re-create `new_page_converted` and `old_page_converted` and find the $(p{'}_{new}$ - $p{'}_{old})$ value 10,000 times using the same simulation process you used in parts (a) through (g) above. \n",
    "\n",
    "We simulate the sampling 10,000 times using `binomial()` giving it the number we want and the probability for each new & old then storing the difference in `p_diffs` array using the same logic we did a moment ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.226362740351693e-06"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling distribution \n",
    "\n",
    "new_con_sim = np.random.binomial(n_new, p_new, 10000)/n_new\n",
    "old_con_sim = np.random.binomial(n_old, p_old, 10000)/n_old\n",
    "p_diffs = new_con_sim - old_con_sim\n",
    "\n",
    "p_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. Histogram**<br> \n",
    "Plot a histogram of the **p_diffs**.  Does this plot look like what you expected?  Use the matching problem in the classroom to assure you fully understand what was computed here.<br>\n",
    "\n",
    "We plot the histogram with the hist function with `p_diffs` as parameters.<br>\n",
    "We draw a vertical line to indicate what our actual difference observed in our original data `df2` using the `plt.axvline()` function.<br>\n",
    "We specify the `xlabel` and `ylabel` and the `legend` to show more info about the graph\n",
    "\n",
    "The histogram is as We expected because it's symmetric about the 0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs)\n",
    "plt.axvline(obs_diff,color='r',label='obs_diff')\n",
    "plt.xlabel('p_diffs')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**j.** What proportion of the **p_diffs** are greater than the actual difference observed in the `df2` data?\n",
    "\n",
    "It's 90.26% of the `p_diffs` that are greater than the `obs_diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.903"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_diffs>obs_diff).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k.** Please explain in words what you have just computed in part **j** above.  \n",
    " - What is this value called in scientific studies?  \n",
    " - What does this value signify in terms of whether or not there is a difference between the new and old pages? *Hint*: Compare the value above with the \"Type I error rate (0.05)\". \n",
    "\n",
    ">90.3% is called a P-Value it's a way to measure The level of statistical significance.<br>\n",
    ">It's insignificant as it's higher than .05 and we fail to reject the null hypothesis.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**l. Using Built-in Methods for Hypothesis Testing**<br>\n",
    "\n",
    "- `convert_old`: number of conversions with the old_page\n",
    "- `convert_new`: number of conversions with the new_page\n",
    "- `n_old`: number of individuals who were shown the old_page\n",
    "- `n_new`: number of individuals who were shown the new_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17489, 17264, 145274, 145310)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# number of conversions with the old_page\n",
    "convert_old = df2.query('landing_page == \"old_page\"')['converted'].sum()\n",
    "\n",
    "# number of conversions with the new_page\n",
    "convert_new =df2.query('landing_page == \"new_page\"')['converted'].sum()\n",
    "\n",
    "# number of individuals who were shown the old_page\n",
    "n_old = df2.query('landing_page == \"old_page\"').shape[0]\n",
    "\n",
    "# number of individuals who received new_page\n",
    "n_new = df2.query('landing_page == \"new_page\"').shape[0]\n",
    "\n",
    "convert_old,convert_new,n_old,n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the p_value using the `proportions_ztest` function providing the numbers of conversions and individuals from the old & new page<br>\n",
    "It's a right-tailed as we defined $H_1$ as $(p_{new} > p_{old})$ so we chose `larger` in the alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3109241984234394 0.9050583127590245\n"
     ]
    }
   ],
   "source": [
    "z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')\n",
    "print(z_score,p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n.** What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages? \n",
    ">P-Value is 0.905 and it's higher than 0.05 .<br>\n",
    "it means that we are 95% confident that the new_page has **not** better conversation rate than the old_page.<br>\n",
    "It's roughly the same as we calculated before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "In this final part, we will see that the result you achieved in the A/B test in Part II above can also be achieved by performing regression.<br><br> \n",
    "\n",
    "**a.** Since each row in the `df2` data is either a conversion or no conversion, what type of regression should you be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> it's a logistic regression as it predicts a whether the value is 1 or 0 (not continuous quantitative values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** The goal is to use **statsmodels** library to fit the regression model you specified in part **a.** above to see if there is a significant difference in conversion based on the page-type a customer receives. However, you first need to create the following two columns in the `df2` dataframe:\n",
    " 1. `intercept` - It should be `1` in the entire column. \n",
    " 2. `ab_page` - It's a dummy variable column, having a value `1` when an individual receives the **treatment**, otherwise `0`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a `copy` of `df2` then we create the `intercept` column with values = 1 <br> \n",
    "We make a boolean mask of the group where the `treatment` equals True then use `apply` to apply whither it's one or not based on treatment.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model=df2.copy()\n",
    "df_model['intercept']=1\n",
    "df_model['ab_page']= (df_model['group']=='treatment').apply(lambda x : 1 if x else 0)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** We Use **statsmodels** to instantiate our regression model on the two columns you created in part (b). \n",
    "\n",
    "We make the model using `Logit` cause it's logistic regression then calling `fit` to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "model = sm.Logit(df_model['converted'],df_model[['ab_page','intercept']])\n",
    "result=model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Provide the summary of your model below, and use it as necessary to answer the following questions.\n",
    "\n",
    "Calling `summary` to find out the P-Value of the ab_page column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 22 Nov 2021</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:18:41</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Mon, 22 Nov 2021   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        17:18:41   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?<br><br>  \n",
    ">P-Value is 0.19 > 0.05. Thus,It's not statistically significant.\n",
    "\n",
    ">It's different from part II cause it was said that it's **one-tailed** test but here it assumes that it's **two-tailed** cause the page could affect with positive or negative so it considers both cases that affect the conversion rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, you are considering other things that might influence whether or not an individual converts. Discuss why it is a good idea to consider other factors to add into your regression model. Are there any disadvantages to adding additional terms into your regression model?\n",
    "\n",
    ">Yes we should consider other factors to help us but watch out if they get too large the more it's complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. Adding countries**<br> \n",
    "Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives in. \n",
    "\n",
    "1. We will need to read in the **countries.csv** dataset and merge together your `df2` datasets on the appropriate rows.then call the resulting dataframe `df_merged`. \n",
    "\n",
    "2. Does it appear that country had an impact on conversion?  To answer this question, We consider the three unique values, `['UK', 'US', 'CA']`, in the `country` column. Creating dummy variables for these country columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the countries using the `read_csv` function and calling `head` to show the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the countries.csv\n",
    "countries=pd.read_csv('countries.csv')\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge our tables with `merge` providing the `countries` DataFrame on the `user_id` column with `left` to the appropriate country for each use on the left DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  \n",
       "0          1        0      US  \n",
       "1          1        0      US  \n",
       "2          1        1      US  \n",
       "3          1        1      US  \n",
       "4          1        0      US  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge with the df_model dataframe\n",
    "df_merged=df_model.merge(countries,on='user_id',how='left')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our `['CA','UK','US']` columns using `get_dummies` to generate wither each column a 1 or 0 then appending to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  \n",
       "0          1        0      US   0   0   1  \n",
       "1          1        0      US   0   0   1  \n",
       "2          1        1      US   0   0   1  \n",
       "3          1        1      US   0   0   1  \n",
       "4          1        0      US   0   0   1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the necessary dummy variables\n",
    "df_merged[['CA','UK','US']]=pd.get_dummies(df_merged['country'])\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the model using `Logit` cause it's logistic regression giving it the converted column as the column we want to predict and our columns for the model to be based on then calling `fit` to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 22 Nov 2021</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:33:07</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0506</td> <td>    0.028</td> <td>    1.784</td> <td> 0.074</td> <td>   -0.005</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>    0.0408</td> <td>    0.027</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.012</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -2.0300</td> <td>    0.027</td> <td>  -76.249</td> <td> 0.000</td> <td>   -2.082</td> <td>   -1.978</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Mon, 22 Nov 2021   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        17:33:07   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "UK             0.0506      0.028      1.784      0.074      -0.005       0.106\n",
       "US             0.0408      0.027      1.516      0.130      -0.012       0.093\n",
       "intercept     -2.0300      0.027    -76.249      0.000      -2.082      -1.978\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.Logit(df_merged['converted'],df_merged[['ab_page','UK','US','intercept']])\n",
    "result=model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countries doesn't affect the conversion rate ; P-values for {UK,US} are above 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h. Fit your model and obtain the results**<br> \n",
    "Though you have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if are there significant effects on conversion.  **Create the necessary additional columns, and fit the new model.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have three countries, we need to include interaction of 2/3 countries.<br>\n",
    "So we make the `US_ab_page` & `UK_ab_page` columns by multiplying the country by if it's new_page or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>US_ab_page</th>\n",
       "      <th>UK_ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  US_ab_page  UK_ab_page  \n",
       "0          1        0      US   0   0   1           0           0  \n",
       "1          1        0      US   0   0   1           0           0  \n",
       "2          1        1      US   0   0   1           1           0  \n",
       "3          1        1      US   0   0   1           1           0  \n",
       "4          1        0      US   0   0   1           0           0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['US_ab_page'] = df_merged['US'] *df_merged['ab_page']\n",
    "df_merged['UK_ab_page'] = df_merged['UK'] *df_merged['ab_page']\n",
    "\n",
    "df_merged.head()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212782.6602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-11-22 17:35</td>       <td>BIC:</td>        <td>212846.1381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290578</td>        <td>LLR p-value:</td>      <td>0.19199</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>-0.0674</td>  <td>0.0520</td>   <td>-1.2967</td> <td>0.1947</td> <td>-0.1694</td> <td>0.0345</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>         <td>0.0118</td>   <td>0.0398</td>   <td>0.2957</td>  <td>0.7674</td> <td>-0.0663</td> <td>0.0899</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>         <td>0.0175</td>   <td>0.0377</td>   <td>0.4652</td>  <td>0.6418</td> <td>-0.0563</td> <td>0.0914</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK_ab_page</th> <td>0.0783</td>   <td>0.0568</td>   <td>1.3783</td>  <td>0.1681</td> <td>-0.0330</td> <td>0.1896</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US_ab_page</th> <td>0.0469</td>   <td>0.0538</td>   <td>0.8718</td>  <td>0.3833</td> <td>-0.0585</td> <td>0.1523</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>-2.0040</td>  <td>0.0364</td>  <td>-55.0077</td> <td>0.0000</td> <td>-2.0754</td> <td>-1.9326</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212782.6602\n",
       "Date:               2021-11-22 17:35 BIC:              212846.1381\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           5                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290578           LLR p-value:      0.19199    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "               Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "ab_page       -0.0674    0.0520   -1.2967  0.1947  -0.1694   0.0345\n",
       "UK             0.0118    0.0398    0.2957  0.7674  -0.0663   0.0899\n",
       "US             0.0175    0.0377    0.4652  0.6418  -0.0563   0.0914\n",
       "UK_ab_page     0.0783    0.0568    1.3783  0.1681  -0.0330   0.1896\n",
       "US_ab_page     0.0469    0.0538    0.8718  0.3833  -0.0585   0.1523\n",
       "intercept     -2.0040    0.0364  -55.0077  0.0000  -2.0754  -1.9326\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Logit to make our model giving it the predicted and independent columns respectively.\n",
    "model = sm.Logit(df_merged['converted'],df_merged[['ab_page','UK','US','UK_ab_page','US_ab_page','intercept']])\n",
    "result=model.fit()\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "P_Value for all of them is higher than 0.05 so it's not statistically significant.\n",
    "\n",
    "The countries relationship to ab_page don't affect the conversion rate ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id =\"Conclusion\">Conclusions</h3>\n",
    "\n",
    "Countries(as an overall factor) don't have an impact on the conversation rate cause P-value for all of them is above .05.\n",
    "\n",
    "The old page is better at the conversation rate  cause from our statistical research we could not reject the null which that the old page has equally or better conversation rate.\n",
    "\n",
    "After a lot of research we found out that there isn't enough that say that the new_page conversion rate is better nor the countries from the logistic regression we modeled<br>\n",
    "So after we found that our sample is very big, we shouldn't continue testing and try making some more new features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
